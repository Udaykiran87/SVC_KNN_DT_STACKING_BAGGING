{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "8c4fd0e6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split , GridSearchCV\n",
    "from sklearn.metrics import accuracy_score , confusion_matrix , roc_auc_score , roc_curve, classification_report\n",
    "from pandas_profiling import ProfileReport\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import pickle\n",
    "import os\n",
    "import csv\n",
    "import logging\n",
    "from pandas_profiling import ProfileReport"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "3ef26d7f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Human_activity_recognition_dt(object):\n",
    "    def __init__(self,dir_path):\n",
    "        self.dir_path = dir_path\n",
    "        logging.basicConfig(filename='dt.log', level=logging.DEBUG,\n",
    "                    format='%(asctime)s:%(levelname)s:%(message)s')\n",
    "        logging.info('Human_activity_recognition_dt class object is created.')\n",
    "        \n",
    "    def prepare_datset(self):\n",
    "        \"\"\"\n",
    "        Create a final csv-'merge.csv'from the directory folder to be used as dataframe for later stage.\n",
    "        \n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        None\n",
    "        \n",
    "        Returns:\n",
    "        ----------\n",
    "        None        \n",
    "        \"\"\"   \n",
    "        logging.info('Dataset preparation started from the raw data.') \n",
    "        try:\n",
    "            # assign directory\n",
    "            directory = self.dir_path\n",
    "\n",
    "            # iterate over files in\n",
    "            # that directory\n",
    "            folder_file_dict = {}\n",
    "            for filename in os.listdir(directory):\n",
    "                f = os.path.join(directory, filename)\n",
    "                # checking if it is not a file\n",
    "                if not os.path.isfile(f):\n",
    "                    file_list = [os.path.join(f, sub_filename) for sub_filename in os.listdir(f) if sub_filename != \"README.txt\"]\n",
    "                    folder_file_dict[filename] = file_list      \n",
    "            header = []\n",
    "            df_list = []\n",
    "            for key in folder_file_dict:\n",
    "                for file in folder_file_dict[key]:\n",
    "                    with open(file, \"r\", encoding=\"shift_jis\", errors=\"\", newline=\"\" ) as f:\n",
    "                        lst = csv.reader(f, delimiter=\",\")\n",
    "                        df = pd.DataFrame(lst)\n",
    "                        df.drop(df.columns[[0,4,5,6,7]], axis=1, inplace =True)\n",
    "                        df_list.append(df)\n",
    "                merged_df = pd.concat(df_list)\n",
    "                merged_df.columns = [\"frontal_axis_reading(g)\",\"vertical_axis_reading(g)\",\"lateral_axis_reading(g)\",\"activity\"]\n",
    "                merged_df.to_csv('merged.csv', index=None, header=True)\n",
    "        except Exception as e:\n",
    "            logging.error(\"{} occured while creating datasets from the raw data.\".format(str(e)))               \n",
    "            \n",
    "    def load_dataset(self):\n",
    "        \"\"\"\n",
    "        Load csv file as pandas dataframe.\n",
    "        \n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        None\n",
    "        \n",
    "        Returns:\n",
    "        ----------\n",
    "        None        \n",
    "        \"\"\"\n",
    "        logging.info('Dataset is getting loaded as pandas dataframe.')\n",
    "        try:        \n",
    "            self.df = pd.read_csv(\"merged.csv\") \n",
    "            self.df.drop(['time','Unnamed: 8'], axis=1, inplace=True)\n",
    "        except FileNotFoundError:\n",
    "            logging.error(\"File not found: exception occured while loading csv as pandas dataframe.\")\n",
    "        except pd.errors.EmptyDataError:\n",
    "            logging.error(\"No data: exception occured while loading csv as pandas dataframe.\")\n",
    "        except pd.errors.ParserError:\n",
    "            logging.errornt(\"Parse error: exception occured while loading csv as pandas dataframe.\")\n",
    "        except Exception as e:\n",
    "            logging.error(\"{} occured while loading csv as pandas dataframe.\".format(str(e)))\n",
    "            \n",
    "    def create_profile_report(self,inp_df):\n",
    "        \"\"\"\n",
    "        Create pandas profile report for the input data frame.\n",
    "        \n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        inp_df: Input data frame.\n",
    "        \n",
    "        Returns:\n",
    "        ----------\n",
    "        None        \n",
    "        \"\"\"    \n",
    "        logging.info('Profile reporting started for dataframe.')\n",
    "        return ProfileReport(inp_df)\n",
    "    \n",
    "    def handle_outlier(self):\n",
    "        \"\"\"\n",
    "        remove outliers for the impacted feature columns.\n",
    "\n",
    "\n",
    "        Parameters\n",
    "        ----------\n",
    "        None\n",
    "\n",
    "        Returns:\n",
    "        ----------\n",
    "        None        \n",
    "        \"\"\"\n",
    "        logging.info('Outliers are getting removed.')\n",
    "        q = self.df['lateral_axis_reading(g)'].quantile(.90)\n",
    "        self.df_new = self.df[self.df['lateral_axis_reading(g)'] < q]\n",
    "        \n",
    "    def standard_scaling(self):\n",
    "        \"\"\"\n",
    "        Perform standard scaling on input dataframe.\n",
    "        \n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        None\n",
    "        \n",
    "        Returns:\n",
    "        ----------\n",
    "        None        \n",
    "        \"\"\"      \n",
    "        logging.info('Standard scalling started for feature columsn.')\n",
    "        self.y = self.df_new['activity']\n",
    "        self.x = self.df_new.drop(columns=['activity'])\n",
    "        scalar = StandardScaler()\n",
    "        self.x_scaled = scalar.fit_transform(self.x)\n",
    "        self.df_new_scalar = pd.DataFrame(scalar.fit_transform(self.df_new))\n",
    "        \n",
    "    def train_test_split(self, test_size, random_state):\n",
    "        \"\"\"\n",
    "        Split data frame into train and test.\n",
    "         \n",
    "        Parameters\n",
    "        ----------\n",
    "        None\n",
    "        \n",
    "        Returns:\n",
    "        ----------\n",
    "        Train and test data for feature and predicted columns.        \n",
    "        \"\"\"\n",
    "        logging.info('train and test split for dataframe started.')\n",
    "        self.x_train, self.x_test, self.y_train, self.y_test = train_test_split(self.x_scaled , self.y , test_size = test_size , random_state = random_state)\n",
    "        \n",
    "    def hyperparam_tuning_fit(self):        \n",
    "        dt = DecisionTreeClassifier()\n",
    "        grid_param = {\"criterion\" : ['gini','entropy'], \n",
    "                      \"splitter\" : ['best','random'],\n",
    "                      \"max_depth\" : range(2,40,1),\n",
    "                      \"min_samples_split\" : range(2,10,1),\n",
    "                      \"min_samples_leaf\" : range(1,10,1),\n",
    "                      \"ccp_alpha\" : np.random.rand(20)\n",
    "                     }\n",
    "\n",
    "        # defining parameter range\n",
    "        self.grid = GridSearchCV(estimator = dt, param_grid = grid_param, n_jobs = -1, cv = 5, refit = True, verbose = 3)\n",
    "        \n",
    "        # fitting the model for grid search\n",
    "        self.dt_model = self.grid.fit(self.x_train, self.y_train)\n",
    "\n",
    "        # print best parameter after tuning\n",
    "        print(self.grid.best_params_)\n",
    "\n",
    "        # print how our model looks after hyper-parameter tuning\n",
    "        print(self.grid.best_estimator_)\n",
    "        \n",
    "    def evaluate_model(self):\n",
    "        \"\"\"\n",
    "        Calculate the classification score.\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        None. \n",
    "        \n",
    "        Returns:\n",
    "        ----------\n",
    "        None. \n",
    "        \"\"\"        \n",
    "        grid_predictions = self.grid.predict(self.x_test)\n",
    "        \n",
    "        accuracy = accuracy_score(self.y_test, grid_predictions)\n",
    "        report = classification_report(self.y_test, grid_predictions)\n",
    "        cm = confusion_matrix(self.y_test, grid_predictions)\n",
    "\n",
    "        print(\"Classification report:\")\n",
    "        print(\"Accuracy: \", accuracy)\n",
    "        print(report)\n",
    "        print(\"Confusion matrix:\")\n",
    "        print(cm)        \n",
    "        \n",
    "    def predict(self,test_case):\n",
    "        \"\"\"\n",
    "        Predict the dependent feature based on the input test case.\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        test_case: It is the independent variable list value. \n",
    "        \n",
    "        Returns:\n",
    "        ----------\n",
    "        Returns the predicted feature. \n",
    "        \"\"\"               \n",
    "        logging.info('Prediction will be done for the testcase {}.'.format(test_case))\n",
    "        try:\n",
    "            return self.grid.predict(test_case)\n",
    "        except Exception as e:\n",
    "            logging.error(\"{} occured while predicting dependent feature.\".format(str(e)))\n",
    "            return None\n",
    "        \n",
    "    def save_dt_model(self,file_name):\n",
    "        \"\"\"\n",
    "        Save the dt model based on the input file name.\n",
    "        \n",
    "        Parameters\n",
    "        ----------\n",
    "        file_name: dt model will be saved with this file name. \n",
    "        \n",
    "        Returns:\n",
    "        ----------\n",
    "        None. \n",
    "        \"\"\"  \n",
    "        logging.info('Save dt model into file: {}.'.format(file_name))\n",
    "        try:\n",
    "            pickle.dump(self.dt_model,open(file_name,'wb'))\n",
    "        except Exception as e:\n",
    "            logging.error(\"{} occured while saving dt model.\".format(str(e)))        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "27b5d72d",
   "metadata": {},
   "outputs": [],
   "source": [
    "dt_obj = Human_activity_recognition_dt('../Datasets_Healthy_Older_People')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "07f9e73f",
   "metadata": {},
   "outputs": [],
   "source": [
    "dt_obj.prepare_datset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5b265ac8",
   "metadata": {},
   "outputs": [],
   "source": [
    "dt_obj.load_dataset()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5d934ff1",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'Human_activity_recognition_dt' object has no attribute 'df'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m~\\AppData\\Local\\Temp/ipykernel_12208/1265062793.py\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0minp_df\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdt_obj\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdf\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[0mdt_obj\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcreate_profile_report\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minp_df\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'Human_activity_recognition_dt' object has no attribute 'df'"
     ]
    }
   ],
   "source": [
    "inp_df = dt_obj.df\n",
    "dt_obj.create_profile_report(inp_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73304a06",
   "metadata": {},
   "outputs": [],
   "source": [
    "dt_obj.handle_outlier()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e19eb7b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "dt_obj.standard_scaling()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28ccab40",
   "metadata": {},
   "outputs": [],
   "source": [
    "dt_obj.train_test_split(0.2,144)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aaa39577",
   "metadata": {},
   "outputs": [],
   "source": [
    "dt_obj.hyperparam_tuning_fit()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4e86902",
   "metadata": {},
   "outputs": [],
   "source": [
    "dt_obj.evaluate_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "151496fd",
   "metadata": {},
   "outputs": [],
   "source": [
    "testcase = [dt_obj.x_test[0]]\n",
    "print(testcase)\n",
    "print(dt_obj.predict(testcase))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "206d2009",
   "metadata": {},
   "outputs": [],
   "source": [
    "dt_obj.save_dt_model('dt_model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0bc107d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
